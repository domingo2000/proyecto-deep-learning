@article{Geirhos_2020,
  doi       = {10.1038/s42256-020-00257-z},
  url       = {https://doi.org/10.1038\%2Fs42256-020-00257-z},
  year      = 2020,
  month     = {nov},
  publisher = {Springer Science and Business Media {LLC}
               },
  volume    = {2},
  number    = {11},
  pages     = {665--673},
  author    = {Robert Geirhos and JÃ¶rn-Henrik Jacobsen and Claudio Michaelis and Richard Zemel and Wieland Brendel and Matthias Bethge and Felix A. Wichmann},
  title     = {Shortcut learning in deep neural networks},
  journal   = {Nature Machine Intelligence}
}

@misc{dasgupta2022distinguishing,
  title         = {Distinguishing rule- and exemplar-based generalization in learning systems},
  author        = {Ishita Dasgupta and Erin Grant and Thomas L. Griffiths},
  year          = {2022},
  eprint        = {2110.04328},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{chan2022transformers,
  title         = {Transformers generalize differently from information stored in context vs in weights},
  author        = {Stephanie C. Y. Chan and Ishita Dasgupta and Junkyung Kim and Dharshan Kumaran and Andrew K. Lampinen and Felix Hill},
  year          = {2022},
  eprint        = {2210.05675},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{lake2018generalization,
  title         = {Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author        = {Brenden M. Lake and Marco Baroni},
  year          = {2018},
  eprint        = {1711.00350},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{Ashish2017Attention,
  author     = {Ashish Vaswani and
                Noam Shazeer and
                Niki Parmar and
                Jakob Uszkoreit and
                Llion Jones and
                Aidan N. Gomez and
                Lukasz Kaiser and
                Illia Polosukhin},
  title      = {Attention Is All You Need},
  journal    = {CoRR},
  volume     = {abs/1706.03762},
  year       = {2017},
  url        = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint     = {1706.03762},
  timestamp  = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{chang2021compositionaltransformers,
  author     = {Yichen Jiang and
                Mohit Bansal},
  title      = {Inducing Transformer's Compositional Generalization Ability via Auxiliary
                Sequence Prediction Tasks},
  journal    = {CoRR},
  volume     = {abs/2109.15256},
  year       = {2021},
  url        = {https://arxiv.org/abs/2109.15256},
  eprinttype = {arXiv},
  eprint     = {2109.15256},
  timestamp  = {Mon, 04 Oct 2021 17:22:25 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2109-15256.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li2019compositionalgeneralizationseq2seq,
  title     = {Compositional Generalization for Primitive Substitutions},
  author    = {Li, Yuanpeng  and
               Zhao, Liang  and
               Wang, Jianyu  and
               Hestness, Joel},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1438},
  doi       = {10.18653/v1/D19-1438},
  pages     = {4293--4302},
  abstract  = {Compositional generalization is a basic mechanism in human language learning, but current neural networks lack such ability. In this paper, we conduct fundamental research for encoding compositionality in neural networks. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the entropy in each representation to improve generalization. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and machine translation. In the SCAN domain, it boosts accuracies from 14.0{\%} to 98.8{\%} in Jump task, and from 92.0{\%} to 99.7{\%} in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning.}
}